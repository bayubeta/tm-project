{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import trange, tqdm\n",
    "#-------------------------------\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg', disable = [\"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\")\n",
    "test_data = pd.read_csv(\"test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([(\"vectorizer\", TfidfVectorizer()), (\"mnb\", MultinomialNB())], verbose=True)\n",
    "pipe.fit(train_data['review'], train_data['sentiment'])\n",
    "predClass = pipe.predict(test_data['review'])\n",
    "\n",
    "print(classification_report(test_data['sentiment'], predClass, target_names = train_data['sentiment'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([(\"vectorizer\", TfidfVectorizer()), (\"logreg\", SGDClassifier(loss = 'log', random_state=1234))], verbose=True)\n",
    "pipe2.fit(train_data['review'], train_data['sentiment'])\n",
    "predClass2 = pipe2.predict(test_data['review'])\n",
    "\n",
    "print(classification_report(test_data['sentiment'], predClass2, target_names = train_data['sentiment'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "pipe3 = Pipeline([(\"vectorizer\", TfidfVectorizer()), (\"SVM\", LinearSVC(random_state=1234))], verbose=True)\n",
    "pipe3.fit(train_data['review'], train_data['sentiment'])\n",
    "predClass3 = pipe3.predict(test_data['review'])\n",
    "\n",
    "print(classification_report(test_data['sentiment'], predClass3, target_names = train_data['sentiment'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class MeanSentenceVectorizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def tokenizer(self, sentence):\n",
    "        doc = nlp(sentence)\n",
    "        preprocessed = [token.text for token in doc]\n",
    "        return preprocessed\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return np.array(\n",
    "            [np.mean([nlp.vocab[word].vector for word in self.tokenizer(sentence)], axis=0) for sentence in tqdm(X)]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe4 = Pipeline([(\"vectorizer\", MeanSentenceVectorizer()), (\"SVM\", LinearSVC(random_state=1234))], verbose=True)\n",
    "pipe4.fit(train_data['review'], train_data['sentiment'])\n",
    "predClass4 = pipe4.predict(test_data['review'])\n",
    "\n",
    "print(classification_report(test_data['sentiment'], predClass4, target_names = train_data['sentiment'].unique()))"
   ]
  },
  {
   "source": [
    "# Using DistilBERT"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, TFDistilBertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertModel: ['vocab_layer_norm', 'vocab_transform', 'vocab_projector', 'activation_13']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = TFDistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# Slice the data for trials\n",
    "r = int(1/2 * len(train_data))\n",
    "\n",
    "tr_d = pd.concat([train_data[:r], train_data[12500:(12500+r)]])\n",
    "te_d = pd.concat([test_data[:r], test_data[12500:(12500+r)]])\n",
    "###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentences(X, maxlen=512):\n",
    "    # tokenize sentences\n",
    "    tokenized = []\n",
    "    for sentence in tqdm(X, desc='Tokenizing sentences'):\n",
    "        token_vec = tokenizer(sentence, return_tensors=\"tf\", truncation=True, padding='max_length', max_length=maxlen)['input_ids']\n",
    "        tokenized.append(token_vec)\n",
    "    tokenized = tf.convert_to_tensor(tf.squeeze(tokenized))\n",
    "\n",
    "    # embedding sentences\n",
    "    vecs = []\n",
    "    batches = list(tf.split(tokenized, 5000))\n",
    "    for batch in tqdm(batches, desc='Processing sentences'):\n",
    "        lhs = model(batch).last_hidden_state[:,0,:]\n",
    "        vecs.append(lhs)\n",
    "    return np.concatenate(vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Tokenizing sentences'), FloatProgress(value=0.0, max=25000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c07809e917bb49a69a48eeafd7e92196"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Processing sentences'), FloatProgress(value=0.0, max=5000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ae48017d52584790aa4f17e81603a349"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_train = embed_sentences(tr_d['review'])\n",
    "np.save('X_train', X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Tokenizing sentences'), FloatProgress(value=0.0, max=25000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "11bbdb342f9e48e9b1f5ecb93533629f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(HTML(value='Processing sentences'), FloatProgress(value=0.0, max=5000.0), HTML(value='')))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61ebe5bc19a64a5886667e62f7ffc7a0"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_test = embed_sentences(te_d['review'])\n",
    "np.save('X_test', X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Bayu\\.conda\\envs\\tm\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.85      0.85     12500\n",
      "    positive       0.85      0.85      0.85     12500\n",
      "\n",
      "    accuracy                           0.85     25000\n",
      "   macro avg       0.85      0.85      0.85     25000\n",
      "weighted avg       0.85      0.85      0.85     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC(random_state=1234).fit(X_train, tr_d['sentiment'])\n",
    "preds = clf.predict(X_test)\n",
    "print(classification_report(te_d['sentiment'], preds, target_names = tr_d['sentiment'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# Using tensorflow:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess Y\n",
    "y_train = np.array([0 if sent == 'negative' else 1 for sent in tqdm(tr_d['sentiment'])])\n",
    "y_test = np.array([0 if sent == 'negative' else 1 for sent in tqdm(te_d['sentiment'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess X\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
    "import string\n",
    "import re\n",
    "\n",
    "def custom_standardization(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n",
    "    return tf.strings.regex_replace(\n",
    "        stripped_html, \"[%s]\" % re.escape(string.punctuation), \"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_layer(X_train)\n",
    "x_val = vectorize_layer(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = len(vectorize_layer.get_vocabulary())\n",
    "embd_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model\n",
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(seed=12345)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(num_words, embd_dim, input_length=x_train.shape[1]))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Conv1D(128, 7, padding=\"valid\", activation=\"relu\", strides=3))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "\n",
    "\"\"\" model.add(Dense(\n",
    "    300, activation='relu', input_shape = (X_train.shape[1],), \n",
    "    kernel_regularizer=l2(1e-5),\n",
    "    bias_regularizer=l2(1e-5),\n",
    "    activity_regularizer=l2(1e-5)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(\n",
    "    Dense(100, activation='relu',\n",
    "    kernel_regularizer=l2(1e-5),\n",
    "    bias_regularizer=l2(1e-5),\n",
    "    activity_regularizer=l2(1e-5)))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid')) \"\"\"\n",
    "\n",
    "\n",
    "model.compile(loss = binary_crossentropy, optimizer = Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size = 125, epochs = 10, verbose = 1, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = vectorize_layer(np.array(te_d['review']))\n",
    "model.evaluate(x_test, y_test, batch_size = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msv = MeanSentenceVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess Y\n",
    "y_train = np.array([0 if sent == 'negative' else 1 for sent in tqdm(tr_d['sentiment'])])\n",
    "y_test = np.array([0 if sent == 'negative' else 1 for sent in tqdm(te_d['sentiment'])])\n",
    "tr_d_msv = msv.transform(tr_d['review'])\n",
    "te_d_msv = msv.transform(te_d['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(tr_d_msv, y_train, test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import binary_crossentropy\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "np.random.seed(seed=12345)\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(x_train.shape[1], activation=\"relu\", input_shape=(x_train.shape[1],),\n",
    "    kernel_regularizer=l2(1e-5),\n",
    "    bias_regularizer=l2(1e-5),\n",
    "    activity_regularizer=l2(1e-5)))\n",
    "model2.add(Dense(x_train.shape[1], activation=\"relu\",\n",
    "    kernel_regularizer=l2(1e-5),\n",
    "    bias_regularizer=l2(1e-5),\n",
    "    activity_regularizer=l2(1e-5)))\n",
    "model2.add(Dense(300, activation=\"relu\",\n",
    "    kernel_regularizer=l2(1e-5),\n",
    "    bias_regularizer=l2(1e-5),\n",
    "    activity_regularizer=l2(1e-5)))\n",
    "model2.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model2.compile(loss = binary_crossentropy, optimizer = Adam(learning_rate=0.01), metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model2.fit(x_train, y_train, batch_size = 125, epochs = 50, verbose = 1, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = te_d_msv\n",
    "model2.evaluate(X_test, y_test, batch_size = 125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}